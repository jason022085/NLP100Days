{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Model(\n",
       "  (layer1): LinearBNAC(\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): Dropout(p=0.3, inplace=False)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): LinearBNAC(\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): Dropout(p=0.3, inplace=False)\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): LinearBNAC(\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "      (1): Dropout(p=0.3, inplace=False)\n",
       "      (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output): LinearBNAC(\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-33345e58fff8>:6: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0882, 0.2048, 0.1534, 0.0998, 0.0962, 0.0606, 0.0412, 0.1108, 0.0855,\n",
      "         0.0595],\n",
      "        [0.1063, 0.1939, 0.0998, 0.1358, 0.0881, 0.0800, 0.0500, 0.1097, 0.0482,\n",
      "         0.0882],\n",
      "        [0.0690, 0.1328, 0.0867, 0.0943, 0.0810, 0.1328, 0.1941, 0.0995, 0.0288,\n",
      "         0.0809],\n",
      "        [0.0449, 0.0660, 0.0672, 0.2198, 0.1115, 0.0882, 0.0773, 0.2004, 0.0401,\n",
      "         0.0846]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.log(output), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0483, -0.0002,  0.0239,  ...,  0.0114,  0.0442,  0.0081],\n",
      "        [-0.0059,  0.0571,  0.0071,  ...,  0.0401,  0.0317, -0.0590],\n",
      "        [ 0.0300, -0.0216, -0.0415,  ..., -0.0303, -0.0282,  0.0401],\n",
      "        ...,\n",
      "        [ 0.0028,  0.0111,  0.0609,  ..., -0.0439,  0.0604, -0.0584],\n",
      "        [-0.0450,  0.0238,  0.0383,  ...,  0.0225,  0.0090, -0.0286],\n",
      "        [-0.0023, -0.0390, -0.0479,  ..., -0.0214,  0.0534,  0.0474]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-1.2451e-02, -1.5396e-03, -1.3489e-02,  ...,  7.5186e-03,\n",
      "         -7.2275e-03,  9.2316e-05],\n",
      "        [-1.0376e-03, -1.3627e-03, -1.8037e-06,  ..., -1.9224e-03,\n",
      "          2.0201e-03, -1.0938e-03],\n",
      "        [-2.3476e-03, -1.9816e-03,  3.9279e-03,  ..., -3.0957e-03,\n",
      "          5.0963e-03, -3.7654e-03],\n",
      "        ...,\n",
      "        [-1.5314e-04, -2.2375e-04,  1.7275e-04,  ..., -2.9125e-04,\n",
      "          3.6786e-04, -2.5134e-04],\n",
      "        [-1.7464e-01, -1.1071e-01, -2.6418e-01,  ...,  2.1880e-02,\n",
      "         -6.7150e-02, -5.8150e-03],\n",
      "        [ 4.3595e-03,  8.5911e-02, -3.8359e-03,  ...,  5.7333e-02,\n",
      "         -4.2210e-02,  4.3548e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0473,  0.0008,  0.0249,  ...,  0.0104,  0.0452,  0.0071],\n",
      "        [-0.0049,  0.0581,  0.0061,  ...,  0.0411,  0.0307, -0.0580],\n",
      "        [ 0.0310, -0.0206, -0.0425,  ..., -0.0293, -0.0292,  0.0411],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0121,  0.0599,  ..., -0.0429,  0.0594, -0.0574],\n",
      "        [-0.0440,  0.0248,  0.0393,  ...,  0.0215,  0.0100, -0.0276],\n",
      "        [-0.0033, -0.0400, -0.0469,  ..., -0.0224,  0.0544,  0.0464]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-1.2451e-02, -1.5396e-03, -1.3489e-02,  ...,  7.5186e-03,\n",
      "         -7.2275e-03,  9.2316e-05],\n",
      "        [-1.0376e-03, -1.3627e-03, -1.8037e-06,  ..., -1.9224e-03,\n",
      "          2.0201e-03, -1.0938e-03],\n",
      "        [-2.3476e-03, -1.9816e-03,  3.9279e-03,  ..., -3.0957e-03,\n",
      "          5.0963e-03, -3.7654e-03],\n",
      "        ...,\n",
      "        [-1.5314e-04, -2.2375e-04,  1.7275e-04,  ..., -2.9125e-04,\n",
      "          3.6786e-04, -2.5134e-04],\n",
      "        [-1.7464e-01, -1.1071e-01, -2.6418e-01,  ...,  2.1880e-02,\n",
      "         -6.7150e-02, -5.8150e-03],\n",
      "        [ 4.3595e-03,  8.5911e-02, -3.8359e-03,  ...,  5.7333e-02,\n",
      "         -4.2210e-02,  4.3548e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0473,  0.0008,  0.0249,  ...,  0.0104,  0.0452,  0.0071],\n",
      "        [-0.0049,  0.0581,  0.0061,  ...,  0.0411,  0.0307, -0.0580],\n",
      "        [ 0.0310, -0.0206, -0.0425,  ..., -0.0293, -0.0292,  0.0411],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0121,  0.0599,  ..., -0.0429,  0.0594, -0.0574],\n",
      "        [-0.0440,  0.0248,  0.0393,  ...,  0.0215,  0.0100, -0.0276],\n",
      "        [-0.0033, -0.0400, -0.0469,  ..., -0.0224,  0.0544,  0.0464]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
